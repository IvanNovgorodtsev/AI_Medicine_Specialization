{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd  \n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.read_csv(\"train_preds.csv\")\n",
    "valid_results = pd.read_csv(\"valid_preds.csv\")\n",
    "\n",
    "# the labels in our dataset\n",
    "class_labels = ['Cardiomegaly',\n",
    " 'Emphysema',\n",
    " 'Effusion',\n",
    " 'Hernia',\n",
    " 'Infiltration',\n",
    " 'Mass',\n",
    " 'Nodule',\n",
    " 'Atelectasis',\n",
    " 'Pneumothorax',\n",
    " 'Pleural_Thickening',\n",
    " 'Pneumonia',\n",
    " 'Fibrosis',\n",
    " 'Edema',\n",
    " 'Consolidation']\n",
    "\n",
    "# the labels for prediction values in our dataset\n",
    "pred_labels = [l + \"_pred\" for l in class_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = valid_results[class_labels].values\n",
    "pred = valid_results[pred_labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peek at our dataset\n",
    "valid_results[np.concatenate([class_labels, pred_labels])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x = class_labels, height= y.sum(axis=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def true_positives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count true positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        TP (int): true positives\n",
    "    \"\"\"\n",
    "    TP = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "\n",
    "    # compute TP\n",
    "    TP = np.sum((y == 1) & (thresholded_preds == 1))\n",
    "    \n",
    "    return TP\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count true negatives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        TN (int): true negatives\n",
    "    \"\"\"\n",
    "    TN = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # compute TN\n",
    "    TN = np.sum((y==0)&(thresholded_preds==0))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return TN\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count false positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        FP (int): false positives\n",
    "    \"\"\"\n",
    "    FP = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # compute FP\n",
    "    FP = np.sum((y==0)& (thresholded_preds==1))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return FP\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count false positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        FN (int): false negatives\n",
    "    \"\"\"\n",
    "    FN = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # compute FN\n",
    "    FN = np.sum((y==1)& (thresholded_preds==0))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we must explicity import 'display' in order for the autograder to compile the submitted code\n",
    "# Even though we could use this function without importing it, keep this import in order to allow the grader to work\n",
    "from IPython.display import display\n",
    "# Test\n",
    "df = pd.DataFrame({'y_test': [1,1,0,0,0,0,0,0,0,1,1,1,1,1],\n",
    "                   'preds_test': [0.8,0.7,0.4,0.3,0.2,0.5,0.6,0.7,0.8,0.1,0.2,0.3,0.4,0],\n",
    "                   'category': ['TP','TP','TN','TN','TN','FP','FP','FP','FP','FN','FN','FN','FN','FN']\n",
    "                  })\n",
    "\n",
    "display(df)\n",
    "#y_test = np.array([1, 0, 0, 1, 1])\n",
    "y_test = df['y_test']\n",
    "\n",
    "#preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "preds_test = df['preds_test']\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"\"\"Our functions calcualted: \n",
    "TP: {true_positives(y_test, preds_test, threshold)}\n",
    "TN: {true_negatives(y_test, preds_test, threshold)}\n",
    "FP: {false_positives(y_test, preds_test, threshold)}\n",
    "FN: {false_negatives(y_test, preds_test, threshold)}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Expected results\")\n",
    "print(f\"There are {sum(df['category'] == 'TP')} TP\")\n",
    "print(f\"There are {sum(df['category'] == 'TN')} TN\")\n",
    "print(f\"There are {sum(df['category'] == 'FP')} FP\")\n",
    "print(f\"There are {sum(df['category'] == 'FN')} FN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_performance_metrics(y, pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_accuracy(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        accuracy (float): accuracy of predictions at threshold\n",
    "    \"\"\"\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # get TP, FP, TN, FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th=0.5)\n",
    "    FP = false_positives(y, pred, th=0.5)\n",
    "    TN = true_negatives(y, pred, th=0.5)\n",
    "    FN = false_negatives(y, pred, th=0.5)\n",
    "\n",
    "    # Compute accuracy using TP, FP, TN, FN\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print('test labels: {y_test}')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\")\n",
    "\n",
    "print(f\"computed accuracy: {get_accuracy(y_test, preds_test, threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(valid_results[\"Emphysema\"].values, np.zeros(len(valid_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_prevalence(y):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "    Returns:\n",
    "        prevalence (float): prevalence of positive cases\n",
    "    \"\"\"\n",
    "    prevalence = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    prevalence = np.sum(y) / len(y)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\\n\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "print(f'test labels: {y_test}')\n",
    "\n",
    "print(f\"computed prevalence: {get_prevalence(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_sensitivity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute sensitivity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n",
    "    \"\"\"\n",
    "    sensitivity = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # get TP and FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th=0.5)\n",
    "    FN = false_negatives(y, pred, th=0.5)\n",
    "\n",
    "    # use TP and FN to compute sensitivity\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "def get_specificity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute specificity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        specificity (float): probability that the test outputs negative given that the case is actually negative\n",
    "    \"\"\"\n",
    "    specificity = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # get TN and FP using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th=0.5)\n",
    "    FP = false_positives(y, pred, th=0.5)\n",
    "    \n",
    "    # use TN and FP to compute specificity \n",
    "    specificity = TN/(FP+TN)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print(f'test labels: {y_test}\\n')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}\\n')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"computed sensitivity: {get_sensitivity(y_test, preds_test, threshold):.2f}\")\n",
    "print(f\"computed specificity: {get_specificity(y_test, preds_test, threshold):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_ppv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute PPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        PPV (float): positive predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    PPV = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # get TP and FP using our previously defined functions\n",
    "    TP = true_positives(y, pred, th=0.5)\n",
    "    FP = false_positives(y, pred, th=0.5)\n",
    "\n",
    "    # use TP and FP to compute PPV\n",
    "    PPV = TP/(TP+FP)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return PPV\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute NPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        NPV (float): negative predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    NPV = 0.0\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # get TN and FN using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th=0.5)\n",
    "    FN = false_negatives(y, pred, th=0.5)\n",
    "\n",
    "    # use TN and FN to compute NPV\n",
    "    NPV = TN/(TN+FN)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return NPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\\n\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print(f'test labels: {y_test}')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}\\n')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"computed ppv: {get_ppv(y_test, preds_test, threshold):.2f}\")\n",
    "print(f\"computed npv: {get_npv(y_test, preds_test, threshold):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_curve(y, pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y[:, c]\n",
    "        df.loc[:, 'pred'] = pred[:, c]\n",
    "        # get positive examples for stratified sampling\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            # stratified sampling of positive and negative examples\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = roc_auc_score(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "    return statistics\n",
    "\n",
    "statistics = bootstrap_auc(y, pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.print_confidence_intervals(class_labels, statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_curve(y, pred, class_labels, curve='prc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def plot_calibration_curve(y, pred):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(class_labels)):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, marker='.')\n",
    "        plt.xlabel(\"Predicted Value\")\n",
    "        plt.ylabel(\"Fraction of Positives\")\n",
    "        plt.title(class_labels[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR \n",
    "\n",
    "y_train = train_results[class_labels].values\n",
    "pred_train = train_results[pred_labels].values\n",
    "pred_calibrated = np.zeros_like(pred)\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    lr = LR(solver='liblinear', max_iter=10000)\n",
    "    lr.fit(pred_train[:, i].reshape(-1, 1), y_train[:, i])    \n",
    "    pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(-1, 1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y[:,], pred_calibrated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
